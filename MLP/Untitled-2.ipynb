{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from keras import optimizers\n",
    "import os\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, MaxPooling1D, LSTM\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import pickle\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "from keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_wrapped_title(ax, title, max_line_length=40, **kwargs):\n",
    "    wrapped_title = \"\\n\".join(textwrap.wrap(title, max_line_length))\n",
    "    ax.set_title(wrapped_title, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_bias_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Bias Error (MBE) between true and predicted values.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        The true values.\n",
    "    y_pred : array-like of shape (n_samples,)\n",
    "        The predicted values.\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        The mean bias error.\n",
    "    \"\"\"\n",
    "    # Convert inputs to NumPy arrays to ensure compatibility\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate the bias errors\n",
    "    bias_errors = y_pred - y_true\n",
    "    \n",
    "    # Calculate the mean of bias errors\n",
    "    mean_bias_error = np.mean(bias_errors)\n",
    "    \n",
    "    return mean_bias_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(dense_layers, dropout_rate, learning_rate, units1, units2, n_feat):\n",
    "    '''\n",
    "    Build the MLP model with grid search parameters\n",
    "    '''\n",
    "    inputs = Input(shape=(n_feat,))\n",
    "    x = inputs\n",
    "    \n",
    "    if dense_layers == 1:\n",
    "        x = Dense(units=units1, activation='relu')(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    elif dense_layers == 2:\n",
    "        x = Dense(units=units2, activation='relu')(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = Dense(units=1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=adam, loss='mean_squared_error')  \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # MLP model \n",
    "\n",
    "# Define the base directory\n",
    "#base_dir = '/rwthfs/rz/cluster/home/rwth1434/DL_EN'\n",
    "base_dir='/Users/annaclara/Documents/E3D/DL-EN'\n",
    "datastore_folder_target_name = 'MLP'\n",
    "destination_folder = os.path.join(base_dir, datastore_folder_target_name)\n",
    "\n",
    "# Define the directory and name of the folder of the results\n",
    "folder_of_results = 'Results'\n",
    "folder_of_results_f = os.path.join(datastore_folder_target_name, folder_of_results)\n",
    "folder_of_results_directory = destination_folder\n",
    "folder_of_results = os.path.join(folder_of_results_directory, folder_of_results)\n",
    "\n",
    "# Create the folder for the results\n",
    "os.makedirs(folder_of_results, exist_ok=True)\n",
    "\n",
    "# Define the file names\n",
    "training_file_name = 'lists_data_MLP.pkl'\n",
    "\n",
    "target_tr_file_name = 'y_tr.csv'\n",
    "target_val_file_name = 'y_val.csv'\n",
    "\n",
    "# Create file paths\n",
    "training_path = os.path.join(destination_folder, training_file_name)\n",
    "target_path_tr = os.path.join(destination_folder, target_tr_file_name)\n",
    "target_path_val = os.path.join(destination_folder, target_val_file_name)\n",
    "\n",
    "# Load data\n",
    "\n",
    "with open(training_path, 'rb') as f:\n",
    "    # Use pickle to load the data dictionary from the file\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "X_tr = loaded_data['X_tr_n']\n",
    "X_val = loaded_data['X_val_n']\n",
    "\n",
    "\n",
    "y_tr = pd.read_csv(target_path_tr)\n",
    "y_tr = y_tr.drop(columns='Unnamed: 0')\n",
    "\n",
    "y_val = pd.read_csv(target_path_val)\n",
    "y_val = y_val.drop(columns='Unnamed: 0')\n",
    "\n",
    "\n",
    "\n",
    "# Scale target data\n",
    "y_tr = np.array(y_tr).reshape(-1, 1)\n",
    "y_val = np.array(y_val).reshape(-1, 1)\n",
    "#y_te = np.array(y_te).reshape(-1, 1)\n",
    "\n",
    "ScalerY = MinMaxScaler()\n",
    "y_tr_n = ScalerY.fit_transform(y_tr)\n",
    "y_val_n = ScalerY.transform(y_val)\n",
    "\n",
    "# Adjust format of input data\n",
    "X_tr= np.array(X_tr)\n",
    "X_val=np.array(X_val)\n",
    "\n",
    "\n",
    "n_feat = X_tr.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for debugging\n",
    "y_tr = y_tr[:5000,:]\n",
    "y_tr_n = y_tr_n[:5000,:]\n",
    "\n",
    "y_val = y_val[:1000,:]\n",
    "y_val_n = y_val_n[:1000,:]\n",
    "\n",
    "X_tr= X_tr[:5000,:]\n",
    "X_val= X_val[:1000,:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 733us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 761us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 747us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 741us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 730us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 651us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 722us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 665us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 678us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 643us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 766us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = KerasRegressor(model=build_model, optimizer='adam', batch_size=660, units1=100, units2=80, dense_layers=1, dropout_rate=0.0, shuffle=False, random_state=1901, verbose=0, learning_rate=0.0001, epochs=200)\n",
    "\n",
    "param_grid = {\n",
    "   'dense_layers': [1, 2],  \n",
    "    'units1': [200, 500, 1000], \n",
    "    'units2': [100, 200, 500],\n",
    "    'dropout_rate': [0.0, 0.2, 0.35],\n",
    "}\n",
    "\n",
    "results = []\n",
    "#####\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = build_model(params['dense_layers'], params['dropout_rate'],  0.0001, params['units1'], params['units2'],  n_feat)\n",
    "    #(dense_layers, dropout_rate, learning_rate, units1, units2, n_feat):\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_tr, y_tr_n, epochs=200, batch_size=660, verbose=0)  # Train with validation data\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_original_scale = ScalerY.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    mae = mean_absolute_error(y_val, y_pred_original_scale)\n",
    "    rmse = root_mean_squared_error(y_val_n, y_pred)\n",
    "    rmse_orig = root_mean_squared_error(y_val, y_pred_original_scale)\n",
    "    mbe = mean_bias_error(y_val, y_pred_original_scale)\n",
    "\n",
    "    # Extract training history\n",
    "    training_loss = history.history['loss']\n",
    "\n",
    "    # Plot learning curve\n",
    "    epochs = range(1, len(training_loss) + 1)\n",
    "    plt.figure(dpi=600)\n",
    "    plt.plot(epochs, training_loss, label='Training MSE')\n",
    "    set_wrapped_title(plt.gca(), f'Training loss (MSE): {params}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = f'learning_curve_{params}.png'  # Define a unique filename based on params\n",
    "    plot_path = os.path.join(folder_of_results, plot_filename)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()  # Close the plot to free up memory\n",
    "\n",
    "    # Plot Prediction vs Target values\n",
    "    plt.figure(dpi=600)\n",
    "    plt.plot(y_pred_original_scale, label = 'Prediction'),\n",
    "    plt.plot(y_val, label = 'Target')\n",
    "    plt.legend()\n",
    "    set_wrapped_title(plt.gca(), f'Prediction and target comparison for {params}', fontsize=14)\n",
    "    plt.ylabel('Energy consumption one hour ahead (wH)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Hide the x-axis\n",
    "    plt.gca().axes.get_xaxis().set_visible(False)\n",
    "    plot_filename2 = f'pred_target{params}.png' \n",
    "    plot_path2 = os.path.join(folder_of_results, plot_filename2)\n",
    "    plt.savefig(plot_path2)\n",
    "    plt.close()\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'params': params,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'rmse_original_scale': rmse_orig,\n",
    "        'mbe': mbe, \n",
    "        'training_time': end_time - start_time,\n",
    "        'learning_curve_plot': plot_path,\n",
    "        'pred_target': plot_path2,\n",
    "        'y_pred_original_scale': y_pred_original_scale\n",
    "    }\n",
    "    results.append(result)\n",
    "    # Store results\n",
    "    result = {\n",
    "        'params': params,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'training_time': end_time - start_time,\n",
    "        'y_pred_original_scale': y_pred_original_scale\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "results_filename = 'Results_MLP_1.pkl'\n",
    "results_path = os.path.join(folder_of_results, results_filename)\n",
    "# Open a file in binary write mode\n",
    "with open(results_path, 'wb') as file:\n",
    "     pickle.dump(results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dropout after every dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
